{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsXyV8TxzOBxkXisPON0Jy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ctc4/ATV-BIGDATA-SPARK/blob/main/COMECANDO_SPARK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRABALHANDO COM SPARK**"
      ],
      "metadata": {
        "id": "_oaajPkwvN3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Iniciando**"
      ],
      "metadata": {
        "id": "9XxEcyMdxuZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "i_gHrhdDvSz1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev--xuKo0IX1",
        "outputId": "aa5a4d1f-1c4f-4c7e-fabc-5951071f255a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "QsUzcXrHJ9AA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "i8ZFUjxbKARX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "r64ryAjgKCpA"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark"
      ],
      "metadata": {
        "id": "P7vJIGXiv0Bl"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfxGFjsbv4Cd",
        "outputId": "6cf6b0e2-6b9e-43b2-8651-92998392c733"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function findspark.init(spark_home=None, python_path=None, edit_rc=False, edit_profile=False)>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Criando instância do PySpark** - estabelecer uma conexão com um cluster"
      ],
      "metadata": {
        "id": "TQptx6Zm37J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext"
      ],
      "metadata": {
        "id": "ekGmi8lcvYmT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_contexto = SparkContext()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "eL9l783W0xAz",
        "outputId": "67101be9-ca16-462d-bef5-c88a7f432f73"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-e3dd9d171c55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark_contexto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    193\u001b[0m             )\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    431\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-9-192ac8514c35>:1 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark_contexto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "EpGQcMxd1SHx",
        "outputId": "5262b6b4-9d51-478d-e9c2-c5a00ed71438"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-04f431a03344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_contexto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'spark_contexto' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Criando uma sessão do Spark**"
      ],
      "metadata": {
        "id": "rZrMYYHBqRR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.getOrCreate()    #create my_spark"
      ],
      "metadata": {
        "id": "Igk4B0Q80All"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark)     #imprime o objeto spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6obeGCjpxe8",
        "outputId": "1d511761-2149-4b72-d38b-7b665f9b9736"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f1368b94ac0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lendo os dados**"
      ],
      "metadata": {
        "id": "dXxBLA3QpSgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.read.csv('/content/sample_data/california_housing_test.csv',inferSchema=True, header =True)"
      ],
      "metadata": {
        "id": "8qAqzTLG3dJN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eb7saY03vZl",
        "outputId": "b0e03493-acc7-46c3-bfd7-4d136a55b6d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(longitude=-122.05, latitude=37.37, housing_median_age=27.0, total_rooms=3885.0, total_bedrooms=661.0, population=1537.0, households=606.0, median_income=6.6085, median_house_value=344700.0)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQvH1B_XreuF",
        "outputId": "ffd3cf9b-b46e-4793-ec3b-dc2e6c094e63"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Criando uma tabela SQL temporária com os dados do Dataset**"
      ],
      "metadata": {
        "id": "Hw1Z_wZ1rkea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.createOrReplaceTempView(\"tabela_temporaria\")"
      ],
      "metadata": {
        "id": "-I6809x1ro4h"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.catalog.listTables()) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHvVu_sErtB7",
        "outputId": "8cae4b7c-09cd-4f39-a05d-d6e568d275ac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fazendo uma consulta SQL**"
      ],
      "metadata": {
        "id": "qrkiWwTtr4zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"FROM tabela_temporaria SELECT longitude, latitude LIMIT 3\" "
      ],
      "metadata": {
        "id": "2l5dDJREr8cp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saida = spark.sql(query)  "
      ],
      "metadata": {
        "id": "Br239yV9r_aS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saida.show() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnEwcZCjsBJK",
        "outputId": "73455581-1c6e-4c89-9161-316285daa192"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|longitude|latitude|\n",
            "+---------+--------+\n",
            "|  -122.05|   37.37|\n",
            "|   -118.3|   34.26|\n",
            "|  -117.81|   33.78|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usando PySpark com Pandas**"
      ],
      "metadata": {
        "id": "IdGIJ2an7O8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"SELECT MAX(total_rooms) as maximo_quartos FROM tabela_temporaria\""
      ],
      "metadata": {
        "id": "eehkbF0Y6qi3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_maximo_quartos = spark.sql(query1)"
      ],
      "metadata": {
        "id": "i7NUz3MV6uFz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_maximo_quartos = q_maximo_quartos.toPandas()"
      ],
      "metadata": {
        "id": "VTQ7rvk96uuV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('A quantidade máxima de quartos é: {}'.format(pd_maximo_quartos['maximo_quartos']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv8lrBPi6wl9",
        "outputId": "bc573110-31fa-49e4-f1d9-ad73de43cfe1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A quantidade máxima de quartos é: 0    30450.0\n",
            "Name: maximo_quartos, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qtd_maximo_quartos = int(pd_maximo_quartos.loc[0,'maximo_quartos'])"
      ],
      "metadata": {
        "id": "o0k81oyp6zM9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = \"SELECT longitude, latitude FROM tabela_temporaria WHERE total_rooms = \"+str(qtd_maximo_quartos)"
      ],
      "metadata": {
        "id": "CSel9ljg684L"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "localizacao_maximo_quartos = spark.sql(query2)"
      ],
      "metadata": {
        "id": "uuhP1-_B6-6U"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_localizacao_maximo_quartos = localizacao_maximo_quartos.toPandas()"
      ],
      "metadata": {
        "id": "8j7Sfyjv7B_0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd_localizacao_maximo_quartos.head()) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cYrwFtC7EfN",
        "outputId": "5064692d-de0f-4719-8b1f-4286abe86e85"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   longitude  latitude\n",
            "0     -117.2     33.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convertendo Pandas DataFrame para Spark DataFrame**"
      ],
      "metadata": {
        "id": "Uqw6azV-GVNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "media = 0\n",
        "desvio_padrao=0.1 \n",
        "pd_temporario = pd.DataFrame(np.random.normal(media,desvio_padrao,100))\n",
        "spark_temporario = spark.createDataFrame(pd_temporario)"
      ],
      "metadata": {
        "id": "poUj-AuXGWy3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NzGye13GnmR",
        "outputId": "d9e2e42e-6332-4d90-d8a7-1f4e532d4cfd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='nova_tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_temporario.createOrReplaceTempView(\"nova_tabela_temporaria\")\n",
        "print(spark.catalog.listTables())   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE8i4spoGq2R",
        "outputId": "68556773-28bf-4ec5-9fb1-fb781ebe23cd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='nova_tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    }
  ]
}